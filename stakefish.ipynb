{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Historical Blocks\n",
    "\n",
    "Loop over each day since Bitcoin genesis and fetch the mined blocks using the Blockchain Data API.\n",
    "\n",
    "The URL format for the Blocks endpoint looks like the following:\n",
    "* Blocks for one day: https://blockchain.info/blocks/$time_in_milliseconds?format=json\n",
    "\n",
    "The data will be saved into a blocks.pkl pickle file, so that it can processed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "# Set the starting timestamp to Jan 8, 2009 in milliseconds\n",
    "start_time = 1231300800 * 1000\n",
    "\n",
    "# Get the current timestamp in milliseconds\n",
    "end_time = int(time.time() * 1000)\n",
    "\n",
    "# Create the data dir if needed\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "\n",
    "# Save our output so we can re-use it\n",
    "with open('./data/blocks.pkl', 'wb') as output:\n",
    "\n",
    "    # Loop through each day fetching the blocks\n",
    "    for timestamp in range(start_time, end_time, 24 * 60 * 60 * 1000):\n",
    "        \n",
    "        print(timestamp)\n",
    "        \n",
    "        # We will try up to 5 times, after that, we give up for this date.\n",
    "        # The missing blocks can be fetched later\n",
    "        for tries in range(5):\n",
    "            # Fetch the data for the current day and append to our results\n",
    "            request = requests.get(f\"https://blockchain.info/blocks/{timestamp}?format=json\")\n",
    "            if request.status_code == 200:\n",
    "                pickle.dump(request.json(), output, pickle.HIGHEST_PROTOCOL)\n",
    "                break\n",
    "            else:\n",
    "                # Wait 100ms and try again\n",
    "                time.sleep(0.1)\n",
    "                print(f\"Failed the request with code: {request.status_code}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch any missing blocks\n",
    "\n",
    "Because the above may not have found all blocks, lets look at the data to see if we are missing any. If so, fetch the missing blocks with the Block Height endpoint of the Blockchain Data API.\n",
    "\n",
    "We'll update the blocks.pkl file with the missing blocks (if there are any) so that we have everything we need.\n",
    "\n",
    "The URL format for the Block Height endpoint looks like the following:\n",
    "* https://blockchain.info/block-height/$block_height?format=json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747747\n",
      "0\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Read in the saved block data\n",
    "blocks = []\n",
    "with open('./data/blocks.pkl', 'rb') as input:\n",
    "    try:\n",
    "        while True:\n",
    "            blocks += pickle.load(input)\n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "# Get the blocks indexes and sort them\n",
    "block_indexes = set(map(lambda block: block['block_index'], blocks))\n",
    "\n",
    "# What is the highest block we've seen?\n",
    "max_index = max(block_indexes)\n",
    "\n",
    "# Get the full range of blocks so that we can determine which, or if, any are missing\n",
    "block_list = set(range(0,max_index+1))\n",
    "\n",
    "# Get the missing block indexes\n",
    "missing_blocks = block_list - block_indexes\n",
    "\n",
    "print(len(blocks))\n",
    "print(len(missing_blocks))\n",
    "print(missing_blocks)\n",
    "\n",
    "blocks_to_add = []\n",
    "\n",
    "# Run through the missing blocks\n",
    "for block in missing_blocks:\n",
    "\n",
    "    # We will try up to 5 times, after that, we give up for this block.\n",
    "    # This block of code can be re-run to try and get any missing blocks again\n",
    "    for tries in range(5):\n",
    "        # Fetch the data for the current day and append to our results\n",
    "        request = requests.get(f\"https://blockchain.info/block-height/{block}?format=json\")\n",
    "        if request.status_code == 200:\n",
    "            result = request.json()\n",
    "            blocks_to_add += result['blocks']\n",
    "            break\n",
    "        else:\n",
    "            # Wait 100ms and try again\n",
    "            time.sleep(0.1)\n",
    "            print(f\"Failed the request with code: {request.status_code}\")\n",
    "\n",
    "# Append these missing blocks to our list\n",
    "blocks += list(map(lambda record: {'hash': record['hash'], 'height': record['height'], 'time': record['time'], 'block_index': record['block_index']},blocks_to_add))\n",
    "\n",
    "# Save our updated output so we can re-use it\n",
    "with open('./data/blocks.pkl', 'wb') as output:\n",
    "    pickle.dump(blocks, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for use\n",
    "\n",
    "Read in our full list of blocks, sort them by their index and compute the time delta between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved block data\n",
    "blocks = []\n",
    "with open('./data/blocks.pkl', 'rb') as input:\n",
    "    try:\n",
    "        while True:\n",
    "            blocks += pickle.load(input)\n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "# Sort the list by block_index\n",
    "sorted_blocks = sorted(blocks, key=lambda block: block['block_index'])\n",
    "\n",
    "# Get the times for each block\n",
    "block_times = list(map(lambda block: block['time'], sorted_blocks))\n",
    "\n",
    "# Calculate the number of minutes between blocks\n",
    "time_diff = [(block_times[i] - block_times[i-1])/60 for i in range(1,len(block_times))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Delta Statistics\n",
    "\n",
    "With this data we are going to see how often we'd expect a block to be separated by its previous block by more than two hours, if the block times had a normal distribution.\n",
    "\n",
    "We are then going to compare that with what actually happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the time between blocks were normally distributed, we'd expect to see two hours or more between blocks one in every 375299968947541 blocks.\n",
      "\n",
      "However, we've actually seen 2 hours or more between blocks on average one in every 4919 blocks, or once every 783 days\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from statistics import mean, stdev\n",
    "\n",
    "# Determine the probability of a 2 hour or greater block if the data was normally distributed.\n",
    "avg = mean(time_diff)\n",
    "cdf = (1+math.erf((120-avg)/stdev(time_diff)/math.sqrt(2)))/2\n",
    "prob = 1 - cdf\n",
    "\n",
    "# Get our actual probability based on the historical data\n",
    "actual = len(list(filter(lambda time: time >= 120, time_diff))) / len(time_diff)\n",
    "\n",
    "days_between_long_blocks = round(avg / actual / 60)\n",
    "\n",
    "print(f\"If the time between blocks were normally distributed, we'd expect to see two hours or more between blocks one in every {int(round(1/prob))} blocks.\")\n",
    "print()\n",
    "print(f\"However, we've actually seen 2 hours or more between blocks on average one in every {int(round(1/actual))} blocks, or once every {days_between_long_blocks} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the number of blocks with 2 hours or more between them\n",
    "\n",
    "Filter this list to only those that had 2 hours or more between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 152 times where consecutive blocks were separated by 2 hours or more.\n"
     ]
    }
   ],
   "source": [
    "# Determine how many times there were blocks separated by more than 2 hours\n",
    "long_block_deltas = len(list(filter(lambda time: time >= 120, time_diff)))\n",
    "\n",
    "print(f\"There were {long_block_deltas} times where consecutive blocks were separated by 2 hours or more.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "025ffbdad8a68dd93d244ee5ce03c7526b1331254238d62f68f9ab778e5a6936"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
